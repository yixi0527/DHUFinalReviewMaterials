1.   1.   有无监督、能否增量学习、基于实例还是基于模型
     2.   有监督、无监督、半监督、强化学习
          1.   前五个有监督，后四个无监督
          2.   监督学习：训练样本和标签的集合；强化学习：计算机与环境的互动
     3.   在线学习、离线、批量学习
     4.   实例

2.   1.   收集数据-输入数据-数据预处理-训练和测试模型-模型的评估

     2.   混淆矩阵

          |          | 实际正例 | 实际负例 |
          | -------- | -------- | -------- |
          | 预测正例 | TP       | FP       |
          | 预测负例 | FN       | TN       |

          标准化 $z = \frac{x-\mu}{\sigma},\mu=\frac{1}{N}\sum\limits_{i=1}^N(x_i),\sigma=\sqrt{\frac{1}{N}\sum\limits_{i=1}^N(x_i-\mu)^2}$

          归一化$x_{norm} = \frac{x-x_{min}}{x_{max}-x_{min}}$

          1.   特征幅度较大，让特征归一到同一个范围中。提升模型的收敛速度（加快梯度下降的求解速度）；提升模型的精度（消除量级和量纲的影响）；简化计算

     3.   准确率

          $Accuracy = \frac{TP+TN}{TP+TN+FP+FN}$

          精确率

          $Precision = \frac{TP}{TP+FP}$

          召回率（查全率）

          $Recall = \frac{TP}{TP+FN}$

          F1分数

          $F1 = \frac{2 \times Precision \times Recall}{Precision + Recall}$ 

          1.   样本不平衡

          2.   不可以。二者两难全，才会出现F1指数。

          3.   留出法：按固定比例将数据集**静态**地划分为训练集、验证集、测试集

               留一法：每次的测试集都只有一个样本，要进行 m 次训练和预测

               k折交叉验证：将数据集分为训练集和测试集；将训练集分为 k 份；每次使用 k 份中的 1 份作为验证集，其他全部作为训练集。

          4.   可以

3.   1.   模型中可被学习和调整的参数，通常是通过训练数据来`自动学习`的，以`最小化损失函数或优化目标`。

          `超参数`则是在算法运行之前手动设置的参数，用于`控制模型的行为和性能`。

     2.   训练速度、收敛性、容量和泛化能力等

     3.   网格搜索、随机搜索；随机搜索。

---

4.   预测时，选取K个最近的邻居
5.   大，小，大，易，交叉验证法

6.   闵可夫斯基距离：$L_p(x_i,x_j) = (\sum^n_{l=1}|x_i^{(l)}-x_j^{(l)}|^p)^\frac{1}{p}$ 

​	$p=1$，曼哈顿距离

​	$p=2$，欧氏距离

​	$p=\infty$，切比雪夫距离。各个坐标距离的最大值。原式会变为$\max\limits_{l}|x_i^{(l)}-x_j^{(l)}|$

7.    

![0bea35f242a16be0c305106d744ddde](./%E8%87%AA%E6%B5%8B%E9%A2%98%E7%AD%94%E6%A1%88.assets/0bea35f242a16be0c305106d744ddde.jpg)

8.   $O(n);O(\log n)$

---

9.   1.   $P(Y|X) = P(Y)\frac{P(X|Y)}{P(X)}$
     2.   $P(Y)$
     3.   $P(Y|X)$
     4.   $P(X)$
     5.   $P(X|Y)$

10.   特征独立性假设

      1.    

           $P(X|Y_i)=P(X^{(1)} = x^{(1)},...,X^{(n)}=x^{(n)}|Y=i) \\ =\mathop{\Pi}\limits_{j=1}\limits^{n} P(X^{(j)} = x^{(j)}|Y = i )$

11.   $y = \arg \max \limits_{i} P(Y=i)\prod\limits_{j}P(X^{(j)}=x^{(j)}|Y = i)$

12.     ![eced73768ce38b98921b4e2f123e4ec](./%E8%87%AA%E6%B5%8B%E9%A2%98%E7%AD%94%E6%A1%88.assets/eced73768ce38b98921b4e2f123e4ec.jpg)

13.   极大似然估计

14.   拉普拉斯平滑

---

15.   损失函数：**单样本**预测的错误程度；

      代价函数：度量**全部样本集**的平均误差；

      目标函数：代价函数和正则化函数，最终要优化的函数

      1.   0-1损失函数、平方损失函数、绝对损失函数、对数损失函数
      2.   均方误差、均方根误差、平均绝对误差

16.   $y = f(x) = w \cdot x + b$​

17.   $J(w) = \frac{1}{2}\sum_{i=1}^N(f(x_i)-y_i)^2 = \frac{1}{2}\sum_{i=1}^{N}(\sum_{k=1}^nw_k\vdot x_i^{(k)}+b-y_i)$

      $J(w) = \frac{1}{2}\sum_{i=1}^{N}(\sum_{k=0}^nw_k\vdot x_i^{(k)}-y_i)$​ 

      也就是均方误差MSE

18.   

      -   初始化参数$w$

      -   $w_j = w_j - \alpha\frac{\partial}{\partial w_j}J(w)$​

      -   重复直到收敛。其中，$\frac{\partial}{\partial w_j}J(w) = \sum \limits_{i=1}^N(f(x_i)-y_i) \vdot x^{(j)}$

19.   $w = (X^TX)^{-1}X^Ty$
20.   L2：岭回归：$\lambda w^2$；L1：套索回归：$\lambda|w|$

21.   L1;L2;L1

---

22.   $f(x)=sign(w\cdot x+b)$​,sign表示返回括号内的数的正负号
23.   $|w\cdot x_i + b|$；$\frac{1}{||w||}|w\cdot x_i + b|$
24.   $-\frac{1}{||w||}y_i(w \cdot x_i + b)$ 这里的符号和$y_i$相当于上式绝对值的作用

25.   函数间隔$\hat\gamma _i = y_i(w\cdot x_i + b)$

26.   $L(w,b)=-\sum\limits_{x_i\in M}y_i(w\cdot x_i+b)$其中，M为误分类点的集合。

27.   任意选择一个超平面$w_0,b_0$

随机选取一个误分类点$(x_i,y_i)$，对$w,b$进行更新。

条件终止：损失函数为0

28.    ![121e09395f58a41fdfcf0f34ba11ef0](./%E8%87%AA%E6%B5%8B%E9%A2%98%E7%AD%94%E6%A1%88.assets/121e09395f58a41fdfcf0f34ba11ef0.jpg)

29.    ![be0e95c7163aaffabdc52c1f1e08572](./%E8%87%AA%E6%B5%8B%E9%A2%98%E7%AD%94%E6%A1%88.assets/be0e95c7163aaffabdc52c1f1e08572.jpg)

---

30.   $z = \sigma(f(x))=\sigma(w\cdot x + b) = \frac{1}{1+e^{-(w \cdot x+b)}}$
31.   极大似然估计
32.   $H(X) = -\sum\limits_{i=1}^n p(x_i)\log(p(x_i))$

33.   0.5
34.   $loss = -\sum\limits_{i=1}^ny_i\log (\hat{y_i})$

35.   交叉熵

36.   $P(Y=1|x) = \pi(x),P(Y=0|x)=1-\pi(x)$

      $\pi(x)=\frac{1}{1+e^{-(w^Tx+b)}}$​

      则似然函数为

      $\prod\limits_{i=1}^{N}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$

      为了得到使似然函数最大的参数，对上式取对数，得到**对数似然函数**

      $L(w) = \sum\limits_{i=1}^N[y_i\log \pi(x_i)+(1-y_i)\log(1-\pi(x_i))]$

      对$L(w)$求极大值，转换为求极小值

      $J(w) = -L(w) = -\sum\limits_{i=1}^N[y_i(w \cdot x_i)-\log(1+e^{w \cdot x_i})]$

      求偏导

      $\frac{\part J(w_j)}{\part w_j} = -\sum\limits_{i=1}^N(y_i-\pi(x_i))\cdot x_i^{(j)}$

      对每个维度j进行更新

      $w_j = w_j - \alpha \frac{\part J(w_j)}{\part w_j} $

37.   随机森林、朴素贝叶斯
38.   一对其余(OvR)或一对全部(OvA)/一对一(OvO)
      1.   N $\frac{N(N-1)}{2}$
      2.   N $\frac{N(N-1)}{2}$
      3.   置信度最大的类别/被预测最多的类别
      4.   OvR相比于OvO：存储开销小、测试时间短、测试时间长

---

39.   $H(Y|X) = \sum\limits_{i=1}^np_iH(Y|X=x_i),p_i = P(X = x_i)$，不确定性

40.   计算特征1和特征2对分类的交叉熵，发现特征2的交叉熵为0，表示特征2使得分类的不确定性为0（或者通过信息增益，显然特征2的信息增益会更大），故特征2更有价值。

41.   信息增益：$g(D,A) = H(D) - H(D|A)$

      信息增益比：$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$。其中$H_A(D)$是数据集$D$关于特征$A$的值的熵。（特征熵，看这一列而不再是预测目标列的信息熵）

      基尼系数：$Gini(p)=\sum\limits_{k=1}^Kp_k(1-p_k)=1-\sum\limits_{k=1}^Kp_k^2$，$Gini(D,A) = p_1Gini(D_1) + p_2Gini(D_2)$

42.   没算完，累死了，看一下重点吧

      ![76fd7a9582c6aa6e04557a40f76548b](./%E8%87%AA%E6%B5%8B%E9%A2%98%E7%AD%94%E6%A1%88.assets/76fd7a9582c6aa6e04557a40f76548b.jpg)![726960e186561030a4a2c878a0334a0](./%E8%87%AA%E6%B5%8B%E9%A2%98%E7%AD%94%E6%A1%88.assets/726960e186561030a4a2c878a0334a0.jpg)

43.   分类标记，所得的分类结果；测试的条件；测试的节点，对数据属性的测试。

44.    

      | 算法 | 支持模型       | 树结构 | 特征选择            | 连续值处理 | 缺失值处理 | 剪枝   | 特征属性多次使用 |
      | ---- | -------------- | ------ | ------------------- | ---------- | ---------- | ------ | ---------------- |
      | ID3  | 分类           | 多叉树 | 信息增益            | 不支持     | 不支持     | 不支持 | 不支持           |
      | C4.5 | 分类           | 多叉树 | 信息增益率          | 支持       | 支持       | 支持   | 不支持           |
      | CART | 分类<br />回归 | 二叉树 | 基尼指数<br/>均方差 | 支持       | 支持       | 支持   | 支持             |

---

45.   离分离超平面最近的点

46.   使支持向量距离分离超平面最远

47.   几何间隔

48.    
      $$
      \begin{align}
      \max\limits_{w,b} & ~ \gamma \\
      s.t. &~ y_i(\frac{w}{||w||}\cdot x_i + \frac{b}{||w||}) \geq \gamma,i=1,2,\cdots,N \\ 
      \end{align}
      $$
      

目标函数：最大化样本点到分离超平面的距离

约束条件：超平面到每个训练样本点的几何间隔至少为$\gamma$​

49.    
      $$
      \begin{align}
      \min\limits_{w,b} & \frac{1}{2}||w||^2 \\
      s.t. & y_i(w\cdot x_i + b) \geq 1,i=1,2,\cdots,N \\ 
      \end{align}
      $$

50.   

50.    

      ![image-20240610235128232](./%E8%87%AA%E6%B5%8B%E9%A2%98%E7%AD%94%E6%A1%88.assets/image-20240610235128232.png)

51.   软间隔
52.   核函数解决低维度线性不可分问题。
53.   

![image-20240610235358402](./%E8%87%AA%E6%B5%8B%E9%A2%98%E7%AD%94%E6%A1%88.assets/image-20240610235358402.png)

---

54.   B

55.   D
56.   C
57.   B
58.   A

59.   样本1：0.25；样本2：0.25；样本3：0.5

60.   更低
61.   A

---

62.   类别数变小时，平均直径会增加。类别数变大超过某个值时，平均直径不变，这个值正是最优的k值。
63.   平坦、非层次化
64.   兰德指数/轮廓系数；[0,1],[-1,1]；都是越大越好
65.   可以用层次聚类对样本进行聚类，得到k个类时停止。然后从每个类中选取一个与中心距离最近的点。

---

66.   先升后降
67.   第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。

68.   计算数据矩阵的**协方差矩阵**

      然后得到**协方差矩阵的特征值和特征向量**

      选择特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵

      这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维

69.    

      ![image-20240611003933827](./%E8%87%AA%E6%B5%8B%E9%A2%98%E7%AD%94%E6%A1%88.assets/image-20240611003933827.png)

![image-20240611003939125](./%E8%87%AA%E6%B5%8B%E9%A2%98%E7%AD%94%E6%A1%88.assets/image-20240611003939125.png)

![image-20240611003947137](./%E8%87%AA%E6%B5%8B%E9%A2%98%E7%AD%94%E6%A1%88.assets/image-20240611003947137.png)
